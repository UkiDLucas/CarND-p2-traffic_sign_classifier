{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile\n",
    "from tqdm import tqdm\n",
    "# conda install -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a wrapper class to show the progrees of the download.\n",
    "class DownloadProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    # This hook function that will be called: \n",
    "    # - once on establishment of the network connection and \n",
    "    # - once after each block read thereafter. \n",
    "    # The hook will receive three arguments: \n",
    "    # - a count of blocks transferred so far, \n",
    "    # - a block size in bytes, and \n",
    "    # - the total size of the file.\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test data downloaded at ../DATA/german_traffic_signs/\n"
     ]
    }
   ],
   "source": [
    "# urllib.request.urlretrieve(url[, filename[, reporthook[, data]]])\n",
    "\n",
    "# I usually keep my DATA one folder below the git repo\n",
    "data_dir = \"../DATA/german_traffic_signs/\"\n",
    "\n",
    "file_name = 'train.p'\n",
    "if not isfile(data_dir + file_name):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='Train Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/udacity-sdc/datasets/german_traffic_sign_benchmark/train.p',\n",
    "            data_dir + file_name,\n",
    "            pbar.hook)\n",
    "\n",
    "        \n",
    "file_name = 'test.p'\n",
    "if not isfile(data_dir + file_name):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='Test Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/udacity-sdc/datasets/german_traffic_sign_benchmark/test.p',\n",
    "            data_dir + file_name,\n",
    "            pbar.hook)\n",
    "        \n",
    "        \n",
    "# Train Dataset: 120MB [01:20, 1.50MB/s]                           \n",
    "# Test Dataset: 38.8MB [00:28, 1.37MB/s]  \n",
    "\n",
    "print('Training and Test data downloaded at ' + data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Switch Keras to TensorFlow (from Theano)\n",
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "print('Modules loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sets are loaded correctly\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + 'train.p', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# TODO: Load the feature data to the variable X_train\n",
    "X_train = data['features']\n",
    "# TODO: Load the label data to the variable y_train\n",
    "y_train = data['labels']\n",
    "\n",
    "# Assert that the data sets loaded correctly\n",
    "assert np.array_equal(X_train, data['features']), 'X_train not set to data[\\'features\\'].'\n",
    "assert np.array_equal(y_train, data['labels']), 'y_train not set to data[\\'labels\\'].'\n",
    "print('Training data sets are loaded correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train) \n",
    "\n",
    "assert X_train.shape == data['features'].shape, 'X_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "assert y_train.shape == data['labels'].shape, 'y_train has changed shape. The shape shouldn\\'t change when shuffling.'\n",
    "assert not np.array_equal(X_train, data['features']), 'X_train not shuffled.'\n",
    "assert not np.array_equal(y_train, data['labels']), 'y_train not shuffled.'\n",
    "print('Tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_color_intensity(image_data):\n",
    "    # desired range\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    # expected values\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dd2dde112668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_color_intensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-512810abdced>\u001b[0m in \u001b[0;36mnormalize_color_intensity\u001b[0;34m(image_data)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgrayscale_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgrayscale_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrayscale_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgrayscale_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgrayscale_min\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_normalized = normalize_color_intensity(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
